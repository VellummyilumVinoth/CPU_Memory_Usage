{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/YASKd88YAb8NMlPDBgf5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VellummyilumVinoth/CPU_Memory_Usage/blob/main/CPU_Memory_Usage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cpu_info = !lscpu\n",
        "for inf_item in cpu_info.get_list():\n",
        "  print(inf_item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtD_XmlRwIKb",
        "outputId": "f35a3781-abf4-43c2-f8a4-c7512a2a5478"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture:                    x86_64\n",
            "CPU op-mode(s):                  32-bit, 64-bit\n",
            "Address sizes:                   39 bits physical, 48 bits virtual\n",
            "Byte Order:                      Little Endian\n",
            "CPU(s):                          8\n",
            "On-line CPU(s) list:             0-7\n",
            "Vendor ID:                       GenuineIntel\n",
            "Model name:                      Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz\n",
            "CPU family:                      6\n",
            "Model:                           142\n",
            "Thread(s) per core:              2\n",
            "Core(s) per socket:              4\n",
            "Socket(s):                       1\n",
            "Stepping:                        12\n",
            "CPU max MHz:                     4200.0000\n",
            "CPU min MHz:                     400.0000\n",
            "BogoMIPS:                        4199.88\n",
            "Flags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust sgx bmi1 avx2 smep bmi2 erms invpcid mpx rdseed adx smap clflushopt intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d arch_capabilities\n",
            "Virtualization:                  VT-x\n",
            "L1d cache:                       128 KiB (4 instances)\n",
            "L1i cache:                       128 KiB (4 instances)\n",
            "L2 cache:                        1 MiB (4 instances)\n",
            "L3 cache:                        6 MiB (1 instance)\n",
            "NUMA node(s):                    1\n",
            "NUMA node0 CPU(s):               0-7\n",
            "Vulnerability Itlb multihit:     KVM: Mitigation: VMX disabled\n",
            "Vulnerability L1tf:              Not affected\n",
            "Vulnerability Mds:               Not affected\n",
            "Vulnerability Meltdown:          Not affected\n",
            "Vulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\n",
            "Vulnerability Retbleed:          Mitigation; Enhanced IBRS\n",
            "Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\n",
            "Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n",
            "Vulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\n",
            "Vulnerability Srbds:             Mitigation; Microcode\n",
            "Vulnerability Tsx async abort:   Not affected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import datetime\n",
        "from transformers import AlbertForMaskedLM,RobertaTokenizerFast\n",
        "from tabulate import tabulate\n",
        "import os\n",
        "import psutil\n",
        "\n",
        "# Start CPU and memory usage monitoring for load the model\n",
        "process = psutil.Process()\n",
        "start_mem = process.memory_info().rss  / 1024 / 1024\n",
        "start_cpu = process.cpu_percent()\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "output_dir = os.path.expanduser(\"./finetuned_albert\")\n",
        "model = AlbertForMaskedLM.from_pretrained(output_dir)\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(output_dir)\n",
        "\n",
        "def predict_masked_token(masked_statement, tokenizer, model):\n",
        "\n",
        "    # Tokenize the masked statement\n",
        "    input_ids = tokenizer.encode(masked_statement, add_special_tokens=False, return_tensors='pt')\n",
        "\n",
        "    # Find the position of the masked token\n",
        "    masked_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1][0].item()\n",
        "\n",
        "    # Generate predictions for the masked token using the fine-tuned model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    # Get the top 5 predictions and their probability scores from the fine-tuned model\n",
        "    probs_ft = torch.nn.functional.softmax(predictions[0, masked_token_index], dim=-1)\n",
        "    top_k_ft = torch.topk(probs_ft, k=5)\n",
        "\n",
        "    # Create a table with the top predictions and their probabilities from both models\n",
        "    table = [[\"Fine-Tuned Model\", f\"{tokenizer.mask_token}\"] + [tokenizer.convert_ids_to_tokens([idx])[0].replace('Ġ', '').lower() for idx in top_k_ft.indices],\n",
        "             [\"Probability\", \"\"] + [f\"{probs_ft[idx].item():.4f}\" for idx in top_k_ft.indices]]\n",
        "\n",
        "    # Print the table\n",
        "    print(tabulate(table, headers=\"firstrow\", tablefmt=\"fancy_grid\"))\n",
        "\n",
        "    return table\n",
        "\n",
        "# Define a sample masked statement\n",
        "masked_statement = \"int <mask> = getAge();\"\n",
        "\n",
        "# Get the current timestamp\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# Start CPU and memory usage monitoring for this function\n",
        "start_cpu1 = process.cpu_percent()\n",
        "start_mem1 = process.memory_info().rss  / 1024 / 1024\n",
        "\n",
        "# Call the function to generate predictions for the masked token\n",
        "table = predict_masked_token(masked_statement, tokenizer, model)\n",
        "\n",
        "# Stop CPU and memory usage monitoring for this function and print the results\n",
        "end_cpu = process.cpu_percent()\n",
        "end_mem = process.memory_info().rss  / 1024 / 1024\n",
        "print(f\"CPU usage for predict_masked_token: {(end_cpu - start_cpu1)/8 :.2f}%\")\n",
        "print(f\"CPU usage for load_model and predict_masked_token: {(end_cpu - start_cpu)/8 :.2f}%\")\n",
        "print(f\"Memory usage for predict_masked_token: {(end_mem - start_mem1) :.2f} MB\")\n",
        "print(f\"Memory usage for load_model and predict_masked_token: {(end_mem - start_mem) :.2f} MB\")\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "\n",
        "# calculate elapsed time in milliseconds\n",
        "elapsed_ms = (end_time.timestamp() - start_time.timestamp()) * 1000\n",
        "\n",
        "print(f\"Time taken for predict_masked_token: {elapsed_ms:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w5WTmIyoKdG",
        "outputId": "79e6ac0e-dbb8-434e-c217-060d292e9814"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒════════════════════╤══════════╤═════════╤════════╤════════╤════════╤══════════╕\n",
            "│ Fine-Tuned Model   │ <mask>   │   check │    int │      s │     bb │   string │\n",
            "╞════════════════════╪══════════╪═════════╪════════╪════════╪════════╪══════════╡\n",
            "│ Probability        │          │  0.0979 │ 0.0768 │ 0.0236 │ 0.0206 │   0.0168 │\n",
            "╘════════════════════╧══════════╧═════════╧════════╧════════╧════════╧══════════╛\n",
            "CPU usage for predict_masked_token: 16.77%\n",
            "CPU usage for load_model and predict_masked_token: 33.92%\n",
            "Memory usage for predict_masked_token: 7.24 MB\n",
            "Memory usage for load_model and predict_masked_token: 95.60 MB\n",
            "Time taken for predict_masked_token: 66.73 ms\n"
          ]
        }
      ]
    }
  ]
}